# ==============================================================
# TikTok Fraud Scoring Experiment (ONE FILE)
# - Generates fake profiles with your field names
# - Trains XGBoost to justify risk-score weights
# - Exports weights, per-user scores, and plots
# ==============================================================

import os, math, json, warnings, random
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.inspection import permutation_importance

import matplotlib.pyplot as plt

try:
    from xgboost import XGBClassifier
except Exception as e:
    raise SystemExit("xgboost not installed. Run: pip install xgboost") from e


# ------------------ CONFIG ------------------
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)

N_USERS = 300           # total fake profiles to generate
OUT_CSV = "fake_tiktok_profiles.csv"
SEED_CLUSTERS = ["cluster_A", "cluster_B", "cluster_C", ""]
GIFT_TYPES = ["rose", "star", "lion", "castle", "galaxy"]
HOME_COUNTRIES = ["SG", "MY", "PH", "ID", "VN"]
# --------------------------------------------


# ------------------ 1) GENERATE PROFILES ------------------
def generate_profiles(n=N_USERS):
    rows = []

    # Mix of profiles: ~30% bad, ~35% ambiguous, ~35% good
    n_bad = max(1, int(0.30 * n))
    n_amb = max(1, int(0.35 * n))
    n_good = n - n_bad - n_amb
    profile_types = (["bad"] * n_bad) + (["ambiguous"] * n_amb) + (["good"] * n_good)
    random.shuffle(profile_types)

    # Simulate a few shared devices for bad/ambiguous clusters
    shared_bad_devices = [f"DEV_{i}" for i in range(1, 8)]      # reused often
    amb_devices_range = list(range(8, 30))
    good_devices_range = list(range(30, 250))

    now = datetime.now()

    for i, ptype in enumerate(profile_types, start=1):
        viewer_id = f"U{i:03d}"
        display_name = f"user{i:03d}"

        # Age
        if ptype == "bad":
            age_days = np.random.randint(0, 10)   # very fresh
        elif ptype == "ambiguous":
            age_days = np.random.randint(5, 60)
        else:
            age_days = np.random.randint(30, 365)

        account_created_ts = (now - timedelta(days=int(age_days))).strftime("%Y-%m-%d %H:%M:%S")

        # Geo vs IP
        geo_home_country = random.choice(HOME_COUNTRIES)
        if ptype == "bad":
            ip_country = random.choice(["PH", "ID", "VN"])
            if ip_country == geo_home_country:
                ip_country = "CN"                 # force mismatch sometimes
        elif ptype == "ambiguous":
            ip_country = random.choice([geo_home_country, "PH", "MY", "ID"])
        else:
            ip_country = geo_home_country

        # Device
        if ptype == "bad":
            device_hash = random.choice(shared_bad_devices)
        elif ptype == "ambiguous":
            device_hash = f"DEV_{random.choice(amb_devices_range)}"
        else:
            device_hash = f"DEV_{random.choice(good_devices_range)}"

        # KYC
        if ptype == "bad":
            kyc_level = random.choice([0, 0, 1])
        elif ptype == "ambiguous":
            kyc_level = random.choice([0, 1, 2])
        else:
            kyc_level = 2

        # Coins / transactions
        if ptype == "bad":
            coins_per_transaction = random.choice([500, 1000, 2000])
            num_transaction = np.random.randint(10, 50)
            avg_session_duration_secs = np.random.randint(10, 120)     # join->gift->leave
            preferred_gift_type = random.choice(["galaxy", "castle", "lion"])
            cluster_id = random.choice(["cluster_A", "cluster_B"])
        elif ptype == "ambiguous":
            coins_per_transaction = random.choice([50, 100, 500])
            num_transaction = np.random.randint(5, 25)
            avg_session_duration_secs = np.random.randint(60, 600)
            preferred_gift_type = random.choice(GIFT_TYPES)
            cluster_id = random.choice(["cluster_C", ""])
        else:
            coins_per_transaction = random.choice([5, 10, 20, 50])
            num_transaction = np.random.randint(1, 12)
            avg_session_duration_secs = np.random.randint(600, 3600)
            preferred_gift_type = random.choice(["rose", "star"])
            cluster_id = ""

        rows.append([
            viewer_id, display_name, account_created_ts, age_days, geo_home_country, ip_country,
            device_hash, kyc_level, coins_per_transaction, num_transaction,
            avg_session_duration_secs, preferred_gift_type, cluster_id, ptype
        ])

    df = pd.DataFrame(rows, columns=[
        "viewer_id", "display_name", "account_created_ts", "age_days", "geo_home_country",
        "ip_country", "device_hash", "kyc_level", "coins_per_transaction", "num_transaction",
        "avg_session_duration_secs", "preferred_gift_type", "cluster_id", "profile_type"
    ])
    return df


df_profiles = generate_profiles(N_USERS)
df_profiles.to_csv(OUT_CSV, index=False)
print(f"✅ Generated {len(df_profiles)} profiles → {OUT_CSV}")
print(df_profiles["profile_type"].value_counts())


# ------------------ 2) DERIVE FEATURES ------------------
df = df_profiles.copy()

# cast/clean
for c in ["age_days","kyc_level","coins_per_transaction","num_transaction","avg_session_duration_secs"]:
    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)

for c in ["cluster_id","preferred_gift_type","geo_home_country","ip_country","device_hash"]:
    df[c] = df[c].fillna("")

# coins/min proxy: big spend + short stay → high velocity
df["coins_per_minute_proxy"] = df["coins_per_transaction"] / (df["avg_session_duration_secs"]/60 + 0.5)

# viewer ratio proxy: simulate viewers in [150, 1200]
viewer_count = np.random.randint(150, 1201, size=len(df))
df["viewer_ratio_proxy"] = df["coins_per_transaction"] / viewer_count

# sync propensity: clusters + expensive gifts
df["is_expensive_gift"] = df["preferred_gift_type"].str.lower().isin(["galaxy","castle","lion"]).astype(int)
df["cluster_flag"] = (df["cluster_id"] != "").astype(int)
df["sync_propensity"] = 0.5*df["cluster_flag"] + 0.5*df["is_expensive_gift"]

# shared device normalised
dev_counts = df.groupby("device_hash")["viewer_id"].transform("count")
df["shared_device_count_norm"] = (dev_counts - dev_counts.min()) / (dev_counts.max() - dev_counts.min() + 1e-9)

# geo mismatch & short session
df["geo_mismatch"] = (df["geo_home_country"] != df["ip_country"]).astype(int)
df["short_session_flag"] = (df["avg_session_duration_secs"] < 120).astype(int)

# spend intensity proxy
df["spend_intensity"] = df["coins_per_transaction"] * (1 + 0.25*df["num_transaction"])
df["spend_intensity_log"] = np.log1p(df["spend_intensity"])

# account age & KYC risk transforms
df["account_age_risk"] = 1 - np.minimum(df["age_days"]/30.0, 1.0)  # new→riskier
df["kyc_risk"] = 1 - (df["kyc_level"]/2.0)                         # 0→risk 1.0, 2→risk 0

# ------------------ 3) SYNTHETIC LABELS ------------------
# probabilistic label based on interpretable rule + noise
z = (
    2.2 * (df["coins_per_minute_proxy"].values / (df["coins_per_minute_proxy"].max() + 1e-9)) +
    1.6 * df["sync_propensity"].values +
    1.6 * (df["viewer_ratio_proxy"].values / (df["viewer_ratio_proxy"].max() + 1e-9)) +
    1.0 * df["account_age_risk"].values +
    1.0 * df["kyc_risk"].values +
    1.6 * df["shared_device_count_norm"].values +
    0.8 * df["short_session_flag"].values +
    0.6 * df["geo_mismatch"].values
)
p = 1 / (1 + np.exp(-(z - 2.5)))                        # shift to avoid trivial classes
p = np.clip(p + np.random.normal(0, 0.05, size=len(p)), 0, 1)
df["label_is_mlaunderer"] = (np.random.rand(len(p)) < p).astype(int)

# ------------------ 4) TRAIN XGBOOST ------------------
feature_cols = [
    "coins_per_minute_proxy",
    "sync_propensity",
    "viewer_ratio_proxy",
    "account_age_risk",
    "kyc_risk",
    "shared_device_count_norm",
    "short_session_flag",
    "geo_mismatch",
    "spend_intensity_log",
    "cluster_flag",
]

X = df[feature_cols].copy()
y = df["label_is_mlaunderer"].astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y
)

model = XGBClassifier(
    n_estimators=300,
    max_depth=3,
    learning_rate=0.1,
    subsample=0.9,
    colsample_bytree=0.9,
    reg_lambda=1.0,
    reg_alpha=0.0,
    objective="binary:logistic",
    eval_metric="logloss",
    random_state=RANDOM_STATE,
)
model.fit(X_train, y_train)

# ------------------ 5) EVALUATE ------------------
y_prob = model.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.5).astype(int)

print("\n=== Holdout Performance ===")
print("AUC:", round(roc_auc_score(y_test, y_prob), 3))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification report:\n", classification_report(y_test, y_pred, digits=3))

# ------------------ 6) EXPLAIN (IMPORTANCES) ------------------
booster = model.get_booster()
gain_scores = booster.get_score(importance_type="gain")    # how much each feature improves model
weight_scores = booster.get_score(importance_type="weight")# split frequency

def scores_to_df(score_dict, name):
    items = [(k, v) for k, v in score_dict.items()]
    df_imp = pd.DataFrame(items, columns=["feature","score"]).sort_values("score", ascending=False)
    df_imp["type"] = name
    return df_imp

df_gain = scores_to_df(gain_scores, "gain")
df_weight = scores_to_df(weight_scores, "weight")

# ensure all features present
for f in feature_cols:
    if f not in df_gain["feature"].values:
        df_gain.loc[len(df_gain)] = [f, 0.0, "gain"]
    if f not in df_weight["feature"].values:
        df_weight.loc[len(df_weight)] = [f, 0.0, "weight"]

df_gain = df_gain.set_index("feature").loc[feature_cols].reset_index()
df_weight = df_weight.set_index("feature").loc[feature_cols].reset_index()

print("\n=== XGB Feature Importance (gain) ===")
print(df_gain)
print("\n=== XGB Feature Importance (weight) ===")
print(df_weight)

# permutation importance (robust, model-agnostic)
perm = permutation_importance(model, X_test, y_test, n_repeats=20, random_state=RANDOM_STATE)
df_perm = pd.DataFrame({
    "feature": feature_cols,
    "perm_importance_mean": perm.importances_mean,
    "perm_importance_std": perm.importances_std
}).sort_values("perm_importance_mean", ascending=False)
print("\n=== Permutation Importance (mean over repeats) ===")
print(df_perm)

# ------------------ 7) ML → SUGGESTED RISK WEIGHTS ------------------
pi = df_perm.set_index("feature")["perm_importance_mean"].reindex(feature_cols).fillna(0).values
if pi.sum() <= 1e-9:
    pi = df_gain["score"].values

pi = np.maximum(pi, 0)
weights = pi / (pi.sum() + 1e-9)
weight_table = pd.DataFrame({"feature": feature_cols, "weight": weights})
weight_table["percent"] = (100 * weight_table["weight"]).round(1)
print("\n=== Suggested Risk Weights (from ML) ===")
print(weight_table)

# ------------------ 8) PER-USER RISK SCORES (0–100) ------------------
X_scaled = X.copy()
mins = X_scaled.min(axis=0)
maxs = X_scaled.max(axis=0)
X_scaled = (X_scaled - mins) / (maxs - mins + 1e-9)
risk_score = 100.0 * X_scaled.values.dot(weights)

df["risk_score"] = risk_score

def band(score):
    if score >= 90: return "Ban"
    if score >= 70: return "Review"
    if score >= 50: return "Monitor"
    return "OK"

df["risk_band"] = df["risk_score"].apply(band)

# ------------------ 9) SAVE OUTPUTS ------------------
out_scores = df[["viewer_id","display_name","risk_score","risk_band"] + feature_cols]\
               .sort_values("risk_score", ascending=False)
out_scores.to_csv("risk_scores.csv", index=False)
weight_table.to_csv("xgb_feature_weights.csv", index=False)

print("\nSaved:")
print(" -", OUT_CSV)
print(" - risk_scores.csv")
print(" - xgb_feature_weights.csv")

# ------------------ 10) PLOTS ------------------
plt.figure()
plt.bar(df_gain["feature"], df_gain["score"])
plt.xticks(rotation=45, ha="right")
plt.title("XGBoost Importance (gain)")
plt.tight_layout()
plt.savefig("xgb_importance_gain.png", dpi=160)

plt.figure()
plt.bar(df_perm["feature"], df_perm["perm_importance_mean"])
plt.xticks(rotation=45, ha="right")
plt.title("Permutation Importance (mean)")
plt.tight_layout()
plt.savefig("xgb_permutation_importance.png", dpi=160)

print("\nExported plots:")
print(" - xgb_importance_gain.png")
print(" - xgb_permutation_importance.png")

# ------------------ 11) SHORT RATIONALE (for judges) ------------------
why = {
    "coins_per_minute_proxy": "High coin velocity indicates laundering throughput.",
    "sync_propensity": "Same-time gifting + clusters suggest organized rings.",
    "viewer_ratio_proxy": "Large gifts with few viewers are anomalous.",
    "account_age_risk": "Fresh accounts spending heavily → burn-and-churn.",
    "kyc_risk": "Low KYC avoids traceability; inherently riskier.",
    "shared_device_count_norm": "Many accounts per device → sock-puppet farms.",
    "short_session_flag": "Join→gift→leave is bot-like, not social.",
    "geo_mismatch": "IP country ≠ home geo → mules/VPN hopping.",
    "spend_intensity_log": "High cumulative spend over many tx is non-casual.",
    "cluster_flag": "Belonging to a known cluster supports ring behavior.",
}
top3 = weight_table.sort_values("weight", ascending=False).head(3)
print("\n=== Rationale (top features) ===")
for f in top3["feature"]:
    print(f"- {f}: {why.get(f, f)}")
